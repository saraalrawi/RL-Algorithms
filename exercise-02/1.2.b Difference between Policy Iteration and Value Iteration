
Policy Iteration: 
Each step of policy iteration involves policy evaluation and policy improvement in order to find an optimal policy.
Policy evaluation uses bellman expectation equation. Policy improvement considers changes at all states and to all possible actions, selecting at each state the action that appears best according to qπ(s, a). In other words, consider the new greedy policy, π. 
The greedy policy takes the action that looks best in the short term—after one step of lookahead—according to vπ.
One drawback to policy iteration is that at each of its iterations involves policy evaluation, which may be a prolonged iterative computation requiring sweeps through the state set.
Value function iteration 
In order to shorten the policy evaluation of the policy iteration without losing convergence guarantees of policy iteration, new algorithm is introduced which is value iteration.
Value iteration uses Bellman optimality equation (control problem). This algorithm can be written as a simple backup operation that combines both the policy improvement and the truncated policy evaluation steps.
In fact, the value iteration backup is identical to the policy evaluation backup except that it requires the maximum to be taken over all actions.
